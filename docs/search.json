[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "In this project, we will be exploring the Diabetes Health Indicators Dataset (available on Kaggle).\nThe data analyzed here was collected via a health-related telephone survey conducted by the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) in 2015. We are interested in finding the best model to predict the occurance of prediabetes or diabetes (a binary variable, Diabetes_binary). That is, we are interested in what risk factors are the most predictive of diabetes, and can we use this subset of variables to classify disease occurrence?\nThere are already well-known medical factors (mainly related to diet) that can increase risk of diabetes, so for this project I am going to focus on some factors that may have less-known associations; mental health, sex, age, education, income, and smoker status.\nThere are many ways to fit a model to a dataset. Here, we split our data into training and test sets and use cross validation to improve the variability of the model performance from one data set to the next. We will look at classification tree modeling and random forest modeling and pick the best model from our options.\nWith a well-fit model, we could potentially create a risk stratification panel for individuals that have not been diagnosed with prediabetes/diabetes.\nIn this file, we will perform perform our model split and fit the data to multiple models with cross validation.\n\n\n\n\n diabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n diabetes &lt;- diabetes %&gt;% \n  mutate(Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n        Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n        Age = factor(Age, levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13), labels = c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80 or older\")),\n        Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"never attended school or only kindergarten\",\"elementary\",\"some high school\",\"high school graduate\",\"some college or technical school\",\"college graduate\")),\n        Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"less than 10k\",\"less than 15k\", \"less than 20k\", \"less than 25k\",\"less than 35k\",\"less than 50k\",\"less than 75k\",\"75 or more\"))\n         ) %&gt;% \n  select(c(Diabetes_binary, Smoker, Sex, Age, Education, Income, MentHlth))\n\n\n\n\nHere, we will use the tidymodels frame work to split our data into training and test sets, with a 0.7/0.3 proportion. Luckily we have many many observations, so we shouldn’t have to worry much about too little data being in the training or test split.\n\nset.seed(10)\ndiab_split&lt;- initial_split(diabetes,prop=0.7)\ndiab_train&lt;- training(diab_split)\ndiab_test&lt;- testing(diab_split)\ndiab_train\n\n# A tibble: 177,576 × 7\n   Diabetes_binary Smoker Sex    Age   Education                 Income MentHlth\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;                     &lt;fct&gt;     &lt;dbl&gt;\n 1 No              No     Male   40-44 college graduate          75 or…        0\n 2 No              No     Female 35-39 college graduate          75 or…        0\n 3 No              Yes    Female 50-54 some college or technica… 75 or…        0\n 4 No              Yes    Male   70-74 high school graduate      less …        0\n 5 No              No     Female 65-69 college graduate          less …        0\n 6 No              No     Male   50-54 some college or technica… 75 or…        3\n 7 No              No     Female 25-29 college graduate          75 or…        2\n 8 No              Yes    Male   50-54 college graduate          75 or…        2\n 9 No              Yes    Female 45-49 some college or technica… less …        0\n10 Yes             No     Female 65-69 college graduate          75 or…        0\n# ℹ 177,566 more rows\n\n\n\n\n\nWhen we split our data, it is randomly split intro training/test. There are chances we may get a weird split by chance, which can make metric evaluation a somewhat variable measurement. Using cross validation can create a less variable measurement of our metric that uses all of our data. With cross validation, no predictions used in the value of the metric were found on data that were used to train that model! Here, we will perform a 5-fold cross validation.\n\ndiab_cv_folds &lt;- vfold_cv(diab_train, 5)\n\n\n\n\nTree models are nonlinear supervised learning models and can be more flexible than linear models. Classification trees, as opposed to regression trees, are used when the response variable is categorical (or binary). With a tree model, we split the predictor space into regions. We than make our prediction based on which ‘bin’ an observation ends up in. The most prevalent class in a bin/region is used as the prediction at that split. These models are easy to interpret, and there are no statistical assumptions necessary to get the fit. Additionally, it has built in variable selection. The con of trees is that small changes in the data can vastly change the tree due to the lack of ‘sharing’ information with nearby data points due to splitting at bins, and there is no optimal algorithm for choosing splits. We also need to use CV to prune the tree/ figure out the optimal size of the tree (a bigger tree may be better, but the computational cost is large).\n\n#Creating our recipe \ntree_rec &lt;- recipe(Diabetes_binary ~., data = diab_train) %&gt;% \n  step_dummy(Age,Income,Sex,Education,Smoker) %&gt;% #create dummy variables for our categorical variables \n  step_normalize(MentHlth) #normalize our numeric variable \n\n#Defining our model and engine\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n=20,\n                          cost_complexity = tune()) %&gt;% \n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n#Creating our workflow \ntree_wkf &lt;- workflow() %&gt;% \n  add_recipe(tree_rec) %&gt;% \n  add_model(tree_mod)\n\n#Using CV to select tuning parameter \ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(2,2))\n\ntree_metrics &lt;- metric_set(mn_log_loss,accuracy) \n\ntree_fits &lt;- tree_wkf %&gt;% \n  tune_grid(resamples = diab_cv_folds,\n            grid = tree_grid,\n            metrics = tree_metrics)\n\ntree_fits %&gt;% collect_metrics() %&gt;% filter(.metric == \"mn_log_loss\")\n\n# A tibble: 4 × 8\n  cost_complexity tree_depth .metric     .estimator  mean     n  std_err .config\n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n1    0.0000000001          1 mn_log_loss binary     0.404     5 0.000720 Prepro…\n2    0.1                   1 mn_log_loss binary     0.404     5 0.000720 Prepro…\n3    0.0000000001         15 mn_log_loss binary     0.379     5 0.000629 Prepro…\n4    0.1                  15 mn_log_loss binary     0.404     5 0.000720 Prepro…\n\n#Selecting the best model's tuning parameters \ntree_best_param&lt;- select_best(tree_fits)\n\nWarning in select_best(tree_fits): No value of `metric` was given;\n\"mn_log_loss\" will be used.\n\n#Finalize the workflow with the best parameters \ntree_final_wkf &lt;- tree_wkf %&gt;% \n  finalize_workflow(tree_best_param)\n\n#Now that we've set up how to fit the final model, will do it on the split object \ntree_final_fit &lt;- tree_final_wkf %&gt;% \n  last_fit(diab_split, metrics = metric_set(accuracy, mn_log_loss))\n\ntree_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.859 Preprocessor1_Model1\n2 mn_log_loss binary         0.380 Preprocessor1_Model1\n\n\nExtracting and our final tree model and examining the variable importance plot:\n\ntree_final_model &lt;- extract_workflow(tree_final_fit) %&gt;% extract_fit_parsnip()\nvip(tree_final_model, geom = \"col\")\n\n\n\n\nIt looks like the biggest variables effecting our model are related to income, mental health, and age. I suspect that income has such a big play as greater income means greater access to healthy foods, health care, and maybe could correlate to be a better work-life balance (access to adequate sleep, exercise, etc.)\n\n\n\nRandom forest models are an ensemble method, meaning they combine many models together and fit multiple different tree models to pseudo replicates of the data set and then combine the outcomes. This method often uses bootstrapping to get multiple models to fit on. This can decrease the variance over an individual tree fit. As mentioned prior, with a basic classification or regression tree model, a branch/split we get can look vastly different from one tree to the next (from one dataset to the next, splits may look very different). Averaging across multiple trees, getting the variance of an average y, improves consistency.\nRandom forest models use bagging (bootstrap aggregation) to create many bootstrap samples, average the bootstrap statistics, and create a bootstrap distribution that mimics the sampling distribution. We randomly select the predictors to use at each split, which decreases correlation between trees. We fit trees to each resample and find predicted y for each, and for classification trees, use the majority vote of the predictions (most common prediction made by all bootstrap trees).\nAveraging many trees can greatly improve prediction, but comes at a loss of interpretability as opposed to normal classification trees.\nIn our model, we will tune on the number of predictors that each split should use. Using fewer predictors at each split will reduce the variance in each tree, but may increase the bias. Using more will reduce bias, but may increase variance and potentially lead to overfitting.\n\n#Using the same recipe as our previous model \nrf_spec &lt;- rand_forest(mtry= tune()) %&gt;% \n  set_engine(\"ranger\", importance = \"permutation\") %&gt;% \n  set_mode(\"classification\")\n\n#Creating workflow \nrf_wkf &lt;- workflow() %&gt;% \n  add_recipe(tree_rec) %&gt;% \n  add_model(rf_spec)\n  \n#Tuning mtry across 5 levels \nrf_grid &lt;- grid_regular(mtry(range = c(3,6)), levels = 2)\n#Fitting to our CV folds \nrf_ft &lt;- rf_wkf %&gt;% \n  tune_grid(resamples = diab_cv_folds,\n            grid = rf_grid,\n            metrics = metric_set(accuracy, mn_log_loss)) #this took about 20 min to run\n\nrf_ft %&gt;% collect_metrics() %&gt;% filter(.metric == \"mn_log_loss\")\n\n# A tibble: 2 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     3 mn_log_loss binary     0.373     5 0.000527 Preprocessor1_Model1\n2     6 mn_log_loss binary     0.370     5 0.000546 Preprocessor1_Model2\n\n#Extract best tuning parameter \nrf_best_param &lt;- select_best(rf_ft, metric = \"mn_log_loss\")\nrf_best_param #The best mtry is 6 predictors \n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     6 Preprocessor1_Model2\n\nrf_final_wkf &lt;- rf_wkf %&gt;% \n  finalize_workflow(rf_best_param)\n\n#refit on the entire training set using this parameter \nrf_final_fit &lt;- rf_final_wkf %&gt;% \n  last_fit(diab_split, metrics = metric_set(accuracy, mn_log_loss))\nrf_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.371 Preprocessor1_Model1\n\n\nExtracting and our final random forest tree model and examining the variable importance plot:\n\n#extract the final model and plot \nrf_final_model &lt;- extract_fit_engine(rf_final_fit)\n\n#VIP\nimpor_data&lt;- as.data.frame(rf_final_model$variable.importance) %&gt;% rownames_to_column(\"Variable\") %&gt;%\n  rename(value = \"rf_final_model$variable.importance\") %&gt;% \n  arrange(desc(value)) \n\nggplot(impor_data, aes(x = reorder(Variable, value), y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() \n\n\n\n\n\n\n\nHere we will compare our best model from the random forest method and the classification tree method on the test set to declare an overall best model.\n\ntree_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.859 Preprocessor1_Model1\n2 mn_log_loss binary         0.380 Preprocessor1_Model1\n\nrf_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.371 Preprocessor1_Model1\n\n\nOur best model using a classification tree had a mean log loss of 0.380 with a tree depth of 15 and cost complexity 1e-10, and declared incoming being greater than $75,000 annually has the most important predictor in the model.\nOur best model using a random forest tree had a mean log loss of 0.371 with mtry = 6 and declared being a college graduate (category of the Education variable) having the highest importance as a predictor, with income greater than $75,000 the second most important.\nOur overall best model, determined via the mean log loss metric, is the random forest tree model."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "",
    "text": "In this project, we will be exploring the Diabetes Health Indicators Dataset (available on Kaggle).\nThe data analyzed here was collected via a health-related telephone survey conducted by the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) in 2015. We are interested in finding the best model to predict the occurance of prediabetes or diabetes (a binary variable, Diabetes_binary). That is, we are interested in what risk factors are the most predictive of diabetes, and can we use this subset of variables to classify disease occurrence?\nThere are already well-known medical factors (mainly related to diet) that can increase risk of diabetes, so for this project I am going to focus on some factors that may have less-known associations; mental health, sex, age, education, income, and smoker status.\nThere are many ways to fit a model to a dataset. Here, we split our data into training and test sets and use cross validation to improve the variability of the model performance from one data set to the next. We will look at classification tree modeling and random forest modeling and pick the best model from our options.\nWith a well-fit model, we could potentially create a risk stratification panel for individuals that have not been diagnosed with prediabetes/diabetes.\nIn this file, we will perform perform our model split and fit the data to multiple models with cross validation."
  },
  {
    "objectID": "Modeling.html#data-import-clean-up",
    "href": "Modeling.html#data-import-clean-up",
    "title": "Modeling",
    "section": "",
    "text": "diabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n diabetes &lt;- diabetes %&gt;% \n  mutate(Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n        Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n        Age = factor(Age, levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13), labels = c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80 or older\")),\n        Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"never attended school or only kindergarten\",\"elementary\",\"some high school\",\"high school graduate\",\"some college or technical school\",\"college graduate\")),\n        Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"less than 10k\",\"less than 15k\", \"less than 20k\", \"less than 25k\",\"less than 35k\",\"less than 50k\",\"less than 75k\",\"75 or more\"))\n         ) %&gt;% \n  select(c(Diabetes_binary, Smoker, Sex, Age, Education, Income, MentHlth))"
  },
  {
    "objectID": "Modeling.html#splitting-the-data",
    "href": "Modeling.html#splitting-the-data",
    "title": "Modeling",
    "section": "",
    "text": "Here, we will use the tidymodels frame work to split our data into training and test sets, with a 0.7/0.3 proportion. Luckily we have many many observations, so we shouldn’t have to worry much about too little data being in the training or test split.\n\nset.seed(10)\ndiab_split&lt;- initial_split(diabetes,prop=0.7)\ndiab_train&lt;- training(diab_split)\ndiab_test&lt;- testing(diab_split)\ndiab_train\n\n# A tibble: 177,576 × 7\n   Diabetes_binary Smoker Sex    Age   Education                 Income MentHlth\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;                     &lt;fct&gt;     &lt;dbl&gt;\n 1 No              No     Male   40-44 college graduate          75 or…        0\n 2 No              No     Female 35-39 college graduate          75 or…        0\n 3 No              Yes    Female 50-54 some college or technica… 75 or…        0\n 4 No              Yes    Male   70-74 high school graduate      less …        0\n 5 No              No     Female 65-69 college graduate          less …        0\n 6 No              No     Male   50-54 some college or technica… 75 or…        3\n 7 No              No     Female 25-29 college graduate          75 or…        2\n 8 No              Yes    Male   50-54 college graduate          75 or…        2\n 9 No              Yes    Female 45-49 some college or technica… less …        0\n10 Yes             No     Female 65-69 college graduate          75 or…        0\n# ℹ 177,566 more rows"
  },
  {
    "objectID": "Modeling.html#cross-validation-folding",
    "href": "Modeling.html#cross-validation-folding",
    "title": "Modeling",
    "section": "",
    "text": "When we split our data, it is randomly split intro training/test. There are chances we may get a weird split by chance, which can make metric evaluation a somewhat variable measurement. Using cross validation can create a less variable measurement of our metric that uses all of our data. With cross validation, no predictions used in the value of the metric were found on data that were used to train that model! Here, we will perform a 5-fold cross validation.\n\ndiab_cv_folds &lt;- vfold_cv(diab_train, 5)"
  },
  {
    "objectID": "Modeling.html#classification-tree-modeling",
    "href": "Modeling.html#classification-tree-modeling",
    "title": "Modeling",
    "section": "",
    "text": "Tree models are nonlinear supervised learning models and can be more flexible than linear models. Classification trees, as opposed to regression trees, are used when the response variable is categorical (or binary). With a tree model, we split the predictor space into regions. We than make our prediction based on which ‘bin’ an observation ends up in. The most prevalent class in a bin/region is used as the prediction at that split. These models are easy to interpret, and there are no statistical assumptions necessary to get the fit. Additionally, it has built in variable selection. The con of trees is that small changes in the data can vastly change the tree due to the lack of ‘sharing’ information with nearby data points due to splitting at bins, and there is no optimal algorithm for choosing splits. We also need to use CV to prune the tree/ figure out the optimal size of the tree (a bigger tree may be better, but the computational cost is large).\n\n#Creating our recipe \ntree_rec &lt;- recipe(Diabetes_binary ~., data = diab_train) %&gt;% \n  step_dummy(Age,Income,Sex,Education,Smoker) %&gt;% #create dummy variables for our categorical variables \n  step_normalize(MentHlth) #normalize our numeric variable \n\n#Defining our model and engine\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n=20,\n                          cost_complexity = tune()) %&gt;% \n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n#Creating our workflow \ntree_wkf &lt;- workflow() %&gt;% \n  add_recipe(tree_rec) %&gt;% \n  add_model(tree_mod)\n\n#Using CV to select tuning parameter \ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(2,2))\n\ntree_metrics &lt;- metric_set(mn_log_loss,accuracy) \n\ntree_fits &lt;- tree_wkf %&gt;% \n  tune_grid(resamples = diab_cv_folds,\n            grid = tree_grid,\n            metrics = tree_metrics)\n\ntree_fits %&gt;% collect_metrics() %&gt;% filter(.metric == \"mn_log_loss\")\n\n# A tibble: 4 × 8\n  cost_complexity tree_depth .metric     .estimator  mean     n  std_err .config\n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n1    0.0000000001          1 mn_log_loss binary     0.404     5 0.000720 Prepro…\n2    0.1                   1 mn_log_loss binary     0.404     5 0.000720 Prepro…\n3    0.0000000001         15 mn_log_loss binary     0.379     5 0.000629 Prepro…\n4    0.1                  15 mn_log_loss binary     0.404     5 0.000720 Prepro…\n\n#Selecting the best model's tuning parameters \ntree_best_param&lt;- select_best(tree_fits)\n\nWarning in select_best(tree_fits): No value of `metric` was given;\n\"mn_log_loss\" will be used.\n\n#Finalize the workflow with the best parameters \ntree_final_wkf &lt;- tree_wkf %&gt;% \n  finalize_workflow(tree_best_param)\n\n#Now that we've set up how to fit the final model, will do it on the split object \ntree_final_fit &lt;- tree_final_wkf %&gt;% \n  last_fit(diab_split, metrics = metric_set(accuracy, mn_log_loss))\n\ntree_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.859 Preprocessor1_Model1\n2 mn_log_loss binary         0.380 Preprocessor1_Model1\n\n\nExtracting and our final tree model and examining the variable importance plot:\n\ntree_final_model &lt;- extract_workflow(tree_final_fit) %&gt;% extract_fit_parsnip()\nvip(tree_final_model, geom = \"col\")\n\n\n\n\nIt looks like the biggest variables effecting our model are related to income, mental health, and age. I suspect that income has such a big play as greater income means greater access to healthy foods, health care, and maybe could correlate to be a better work-life balance (access to adequate sleep, exercise, etc.)"
  },
  {
    "objectID": "Modeling.html#random-forest-modeling",
    "href": "Modeling.html#random-forest-modeling",
    "title": "Modeling",
    "section": "",
    "text": "Random forest models are an ensemble method, meaning they combine many models together and fit multiple different tree models to pseudo replicates of the data set and then combine the outcomes. This method often uses bootstrapping to get multiple models to fit on. This can decrease the variance over an individual tree fit. As mentioned prior, with a basic classification or regression tree model, a branch/split we get can look vastly different from one tree to the next (from one dataset to the next, splits may look very different). Averaging across multiple trees, getting the variance of an average y, improves consistency.\nRandom forest models use bagging (bootstrap aggregation) to create many bootstrap samples, average the bootstrap statistics, and create a bootstrap distribution that mimics the sampling distribution. We randomly select the predictors to use at each split, which decreases correlation between trees. We fit trees to each resample and find predicted y for each, and for classification trees, use the majority vote of the predictions (most common prediction made by all bootstrap trees).\nAveraging many trees can greatly improve prediction, but comes at a loss of interpretability as opposed to normal classification trees.\nIn our model, we will tune on the number of predictors that each split should use. Using fewer predictors at each split will reduce the variance in each tree, but may increase the bias. Using more will reduce bias, but may increase variance and potentially lead to overfitting.\n\n#Using the same recipe as our previous model \nrf_spec &lt;- rand_forest(mtry= tune()) %&gt;% \n  set_engine(\"ranger\", importance = \"permutation\") %&gt;% \n  set_mode(\"classification\")\n\n#Creating workflow \nrf_wkf &lt;- workflow() %&gt;% \n  add_recipe(tree_rec) %&gt;% \n  add_model(rf_spec)\n  \n#Tuning mtry across 5 levels \nrf_grid &lt;- grid_regular(mtry(range = c(3,6)), levels = 2)\n#Fitting to our CV folds \nrf_ft &lt;- rf_wkf %&gt;% \n  tune_grid(resamples = diab_cv_folds,\n            grid = rf_grid,\n            metrics = metric_set(accuracy, mn_log_loss)) #this took about 20 min to run\n\nrf_ft %&gt;% collect_metrics() %&gt;% filter(.metric == \"mn_log_loss\")\n\n# A tibble: 2 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     3 mn_log_loss binary     0.373     5 0.000527 Preprocessor1_Model1\n2     6 mn_log_loss binary     0.370     5 0.000546 Preprocessor1_Model2\n\n#Extract best tuning parameter \nrf_best_param &lt;- select_best(rf_ft, metric = \"mn_log_loss\")\nrf_best_param #The best mtry is 6 predictors \n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     6 Preprocessor1_Model2\n\nrf_final_wkf &lt;- rf_wkf %&gt;% \n  finalize_workflow(rf_best_param)\n\n#refit on the entire training set using this parameter \nrf_final_fit &lt;- rf_final_wkf %&gt;% \n  last_fit(diab_split, metrics = metric_set(accuracy, mn_log_loss))\nrf_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.371 Preprocessor1_Model1\n\n\nExtracting and our final random forest tree model and examining the variable importance plot:\n\n#extract the final model and plot \nrf_final_model &lt;- extract_fit_engine(rf_final_fit)\n\n#VIP\nimpor_data&lt;- as.data.frame(rf_final_model$variable.importance) %&gt;% rownames_to_column(\"Variable\") %&gt;%\n  rename(value = \"rf_final_model$variable.importance\") %&gt;% \n  arrange(desc(value)) \n\nggplot(impor_data, aes(x = reorder(Variable, value), y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip()"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling",
    "section": "",
    "text": "Here we will compare our best model from the random forest method and the classification tree method on the test set to declare an overall best model.\n\ntree_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.859 Preprocessor1_Model1\n2 mn_log_loss binary         0.380 Preprocessor1_Model1\n\nrf_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.371 Preprocessor1_Model1\n\n\nOur best model using a classification tree had a mean log loss of 0.380 with a tree depth of 15 and cost complexity 1e-10, and declared incoming being greater than $75,000 annually has the most important predictor in the model.\nOur best model using a random forest tree had a mean log loss of 0.371 with mtry = 6 and declared being a college graduate (category of the Education variable) having the highest importance as a predictor, with income greater than $75,000 the second most important.\nOur overall best model, determined via the mean log loss metric, is the random forest tree model."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "In this project, we will be exploring the Diabetes Health Indicators Dataset (available on Kaggle).\nDiabetes is a serious chronic disease with multiple complications associated. There are many genetic, environmental, and lifestyle factors that can be associated with risk of diabetes. The risk of type II diabetes, the most common form, can differ by race, education, age, income, and many additional factors.\nThe data analyzed here was collected via a health-related telephone survey conducted by the CDC’s Behavioral Risk Factor Surveillance System (BRFSS). These features are either questions directly answered by participants or calculated variables based on participants’ responses. We are interested in finding the best model to predict the occurance of prediabetes or diabetes (a binary variable, Diabetes_binary). That is, we are interested in what risk factors are the most predictive of diabetes, and can we use this subset of variables to classify disease occurrence?\nThere are already well-known medical factors (mainly related to diet) that can increase risk of diabetes, so for this project I am going to focus on some factors that may have less-known associations; mental health, sex, age, education, income, and smoker status.\nEspecially because we did not personally create this data, the purpose of this exploratory analysis is to get to know the data and how it is stored. This includes overall distributions of variables, how variables are stored (numerically, as factor variables, etc- do the column types make sense for the variable) and any pre-existing relationships between variables that we should be aware of before attempting to fit the data to a predictive model.\nWith a well-fit model, we could potentially create a risk stratification panel for individuals that have not been diagnosed with prediabetes/diabetes.\nIn this file, we will perform exploratory data analysis before moving on to modeling with the data.\n\n\n\nClick here for the codebook!\n\n\nVariable\nData Type\nInformation\n\n\n\n\nDiabetes_binary\nbinary\n0 = no diabetes 1 = prediabetes or diabetes\n\n\nSmoker\nbinary\nHave you smoked at least 100 cigarettes in your entire life? 0 = no 1 = yes\n\n\nMentHlth\ninteger/numeric\nFor how many days during the past 30 days was your mental health not good? scale 1-30 days\n\n\nSex\nbinary\n0 = female 1 = male\n\n\nAge\ninteger\n13-level age category (_AGEG5YR see codebook) 1 = 18-24 9 = 60-64 13 = 80 or older\n\n\nEducation\ninteger/factor\nEducation level (EDUCA see codebook) scale 1-6\n\n\nIncome\ninteger/factor\nIncome scale (INCOME2 see codebook)\n\n\n\n\n\n\n\n\n diabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nstr(diabetes)\n\nspc_tbl_ [253,680 × 22] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Diabetes_binary     : num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num [1:253680] 1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num [1:253680] 1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num [1:253680] 1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num [1:253680] 0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num [1:253680] 1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num [1:253680] 0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num [1:253680] 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num [1:253680] 1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num [1:253680] 0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num [1:253680] 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Diabetes_binary = col_double(),\n  ..   HighBP = col_double(),\n  ..   HighChol = col_double(),\n  ..   CholCheck = col_double(),\n  ..   BMI = col_double(),\n  ..   Smoker = col_double(),\n  ..   Stroke = col_double(),\n  ..   HeartDiseaseorAttack = col_double(),\n  ..   PhysActivity = col_double(),\n  ..   Fruits = col_double(),\n  ..   Veggies = col_double(),\n  ..   HvyAlcoholConsump = col_double(),\n  ..   AnyHealthcare = col_double(),\n  ..   NoDocbcCost = col_double(),\n  ..   GenHlth = col_double(),\n  ..   MentHlth = col_double(),\n  ..   PhysHlth = col_double(),\n  ..   DiffWalk = col_double(),\n  ..   Sex = col_double(),\n  ..   Age = col_double(),\n  ..   Education = col_double(),\n  ..   Income = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# all variables have been imported as numeric, going to adjust appropriately- \n\ndiabetes &lt;- diabetes %&gt;% \n  mutate(Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n        Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n        Age = factor(Age, levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13), labels = c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80 or older\")),\n        Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"never attended school or only kindergarten\",\"elementary\",\"some high school\",\"high school graduate\",\"some college or technical school\",\"college graduate\")),\n        Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"less than 10k\",\"less than 15k\", \"less than 20k\", \"less than 25k\",\"less than 35k\",\"less than 50k\",\"less than 75k\",\"75 or more\"))\n         ) %&gt;% \n  select(c(Diabetes_binary, Smoker, Sex, Age, Education, Income, MentHlth))\n\n#Checking for missingness \nsum(is.na(diabetes))\n\n[1] 0\n\n  #no NAs \n\ndescribe(diabetes)\n\n                 vars      n mean   sd median trimmed  mad min max range  skew\nDiabetes_binary*    1 253680 1.14 0.35      1    1.05 0.00   1   2     1  2.08\nSmoker*             2 253680 1.44 0.50      1    1.43 0.00   1   2     1  0.23\nSex*                3 253680 1.44 0.50      1    1.43 0.00   1   2     1  0.24\nAge*                4 253680 8.03 3.05      8    8.17 2.97   1  13    12 -0.36\nEducation*          5 253680 5.05 0.99      5    5.15 1.48   1   6     5 -0.78\nIncome*             6 253680 6.05 2.07      7    6.35 1.48   1   8     7 -0.89\nMentHlth            7 253680 3.18 7.41      0    1.04 0.00   0  30    30  2.72\n                 kurtosis   se\nDiabetes_binary*     2.34 0.00\nSmoker*             -1.95 0.00\nSex*                -1.94 0.00\nAge*                -0.58 0.01\nEducation*           0.04 0.00\nIncome*             -0.28 0.00\nMentHlth             6.44 0.01\n\n #note, all the categorical variables have * next to the variable name.\n\n\n\n\n\n\n\ndiabetes %&gt;% group_by(Diabetes_binary) %&gt;% summarize(count=n())\n\n# A tibble: 2 × 2\n  Diabetes_binary  count\n  &lt;fct&gt;            &lt;int&gt;\n1 No              218334\n2 Yes              35346\n\n\n\ndiabetes %&gt;% group_by(Diabetes_binary,Smoker) %&gt;% summarize(count=n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary Smoker  count\n  &lt;fct&gt;           &lt;fct&gt;   &lt;int&gt;\n1 No              No     124228\n2 No              Yes     94106\n3 Yes             No      17029\n4 Yes             Yes     18317\n\nggplot(data = diabetes, aes(x=Smoker, fill=Diabetes_binary)) +\n  geom_bar() +\n  labs(x=\"Smoker Status\") +\n  scale_fill_discrete(\"Diabetes\")\n\n\n\n\nThere doesn’t appear to be any notable association here.\n\ndiabetes %&gt;% group_by(Diabetes_binary,Age) %&gt;% summarize(count=n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 26 × 3\n# Groups:   Diabetes_binary [2]\n   Diabetes_binary Age   count\n   &lt;fct&gt;           &lt;fct&gt; &lt;int&gt;\n 1 No              18-24  5622\n 2 No              25-29  7458\n 3 No              30-34 10809\n 4 No              35-39 13197\n 5 No              40-44 15106\n 6 No              45-49 18077\n 7 No              50-54 23226\n 8 No              55-59 26569\n 9 No              60-64 27511\n10 No              65-69 25636\n# ℹ 16 more rows\n\nggplot(data = diabetes, aes(x=Age, fill=Diabetes_binary)) +\n  geom_bar() +\n  labs(x=\"Age, 5 year bins\") +\n  scale_fill_discrete(\"Diabetes\")\n\n\n\n\nI expected diabetes occurance to have the positive trend with age. It visually looks like it goes down at 70+ years old, but I believe this is just due to the total N of these age groups being lower.\n\ndiabetes %&gt;% group_by(Diabetes_binary,Sex) %&gt;% summarize(count=n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary Sex     count\n  &lt;fct&gt;           &lt;fct&gt;   &lt;int&gt;\n1 No              Female 123563\n2 No              Male    94771\n3 Yes             Female  18411\n4 Yes             Male    16935\n\nggplot(data = diabetes, aes(x=Sex, fill=Diabetes_binary)) +\n  geom_bar() +\n  labs(x=\"Sex\") +\n  scale_fill_discrete(\"Diabetes\")\n\n\n\n\nNo big difference here.\n\ndiabetes %&gt;% group_by(Diabetes_binary,Income) %&gt;% summarize(count=n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 16 × 3\n# Groups:   Diabetes_binary [2]\n   Diabetes_binary Income        count\n   &lt;fct&gt;           &lt;fct&gt;         &lt;int&gt;\n 1 No              less than 10k  7428\n 2 No              less than 15k  8697\n 3 No              less than 20k 12426\n 4 No              less than 25k 16081\n 5 No              less than 35k 21379\n 6 No              less than 50k 31179\n 7 No              less than 75k 37954\n 8 No              75 or more    83190\n 9 Yes             less than 10k  2383\n10 Yes             less than 15k  3086\n11 Yes             less than 20k  3568\n12 Yes             less than 25k  4054\n13 Yes             less than 35k  4504\n14 Yes             less than 50k  5291\n15 Yes             less than 75k  5265\n16 Yes             75 or more     7195\n\nggplot(data = diabetes, aes(x=Income, fill=Diabetes_binary)) +\n  geom_bar() +\n  labs(x=\"Income\") +\n  scale_fill_discrete(\"Diabetes\")\n\n\n\n\nOne thing you could glean here is that there are over double the amount of people that make more than 75k versus less than 75k (and every other wealth status for that matter) but it doesn’t appear that diabetes occurance doubles- looks like income could potentially be a protective factor to a certain extent. This would make sense; better access to healthier foods, health care, etc.\n\ndiabetes %&gt;% group_by(Diabetes_binary,Education) %&gt;% summarize(count=n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 12 × 3\n# Groups:   Diabetes_binary [2]\n   Diabetes_binary Education                                  count\n   &lt;fct&gt;           &lt;fct&gt;                                      &lt;int&gt;\n 1 No              never attended school or only kindergarten   127\n 2 No              elementary                                  2860\n 3 No              some high school                            7182\n 4 No              high school graduate                       51684\n 5 No              some college or technical school           59556\n 6 No              college graduate                           96925\n 7 Yes             never attended school or only kindergarten    47\n 8 Yes             elementary                                  1183\n 9 Yes             some high school                            2296\n10 Yes             high school graduate                       11066\n11 Yes             some college or technical school           10354\n12 Yes             college graduate                           10400\n\nggplot(data = diabetes, aes(x=Education, fill=Diabetes_binary)) +\n  geom_bar() +\n  labs(x=\"Education\") +\n  scale_fill_discrete(\"Diabetes\")\n\n\n\n\nNo notable associatons here. I wish they were able to collect a more equal amount of people across all education statuses.\n\ndiabetes %&gt;% group_by(Diabetes_binary) %&gt;% summarize(mean_MentHlth=mean(MentHlth),median_MentHlth=median(MentHlth))\n\n# A tibble: 2 × 3\n  Diabetes_binary mean_MentHlth median_MentHlth\n  &lt;fct&gt;                   &lt;dbl&gt;           &lt;dbl&gt;\n1 No                       2.98               0\n2 Yes                      4.46               0\n\nggplot(data = diabetes) +\n  geom_boxplot(aes(x = Diabetes_binary, y=MentHlth, fill=Diabetes_binary)) +\n  labs(y=\"Mental Health, Rated 1-30 Days Feeling Poorly\")\n\n\n\n\nIt looks like those with diabetes have experienced more poor mental health days than those without, which makes sense. It would be interesting to see that variable further split up- what caused that poor mental health (job? physical health? relationship stress?)\n\n\n\n\nggplot(data=diabetes) +\n  geom_boxplot(aes(x=Income, y=MentHlth, fill = Income))\n\n\n\n\nIt looks like the median of poor mental health days decreases as income levels increase. Lower-income groups have a higher spread/variability than higher-income groups. In general, higher income appears to correlate with fewer days of poor mental health on average.\n\nggplot(data=diabetes) +\n  geom_boxplot(aes(x=Education, y=MentHlth, fill = Education))\n\n\n\n\nWe see similar trends here; it appears that the median amount of poor days decreases as education levels increase.\n\nmosaicplot(table(diabetes$Income, diabetes$Education), \n           color = TRUE, \n           main = \"Mosaic Plot of Income and Education\",\n           xlab = \"Income\", \n           ylab = \"Education\")\n\n\n\n\nEducation and income appear to have a positive correlation.\n\nmosaicplot(table(diabetes$Smoker, diabetes$Sex), \n           color = TRUE, \n           main = \"Mosaic Plot of Smoker status and Sex\",\n           xlab = \"Smoker\", \n           ylab = \"Sex\")\n\n\n\n\nIn this population if you are a male you are more likely to be a smoker than if you are a female.\n\nggplot(data = diabetes, aes(x=Sex, fill =Smoker)) +\n  geom_bar(position = \"dodge\") +\n  labs(x=\"Sex\") +\n  scale_fill_discrete(\"Smoker\") +\n  facet_wrap(~ Education)\n\n\n\n\nHere we see that for lower education levels (elementary, some highschool, highschool graduate, some college) there are more male smokers than female (relative to the total count in each group- there is a higher percentage of men that smoke that dont) but in the highest education level, for both sexes it is less common to be a smoker.\n\nggplot(data = diabetes, aes(x=Sex, fill =Smoker)) +\n  geom_bar(position = \"dodge\") +\n  labs(x=\"Sex\") +\n  scale_fill_discrete(\"Smoker\") +\n  facet_wrap(~ Income)\n\n\n\n\nWe see the same trends here- again highlighting the association between income and education it appears.\n\n\n\n\nThe biggest thing we have gathered from this analysis is that there is an association between two predictor variables, education and income. There are likely other positive associations that just make logical sense (like age and smoking, or age and education or income level). We also saw some potential associations between mental health and other predictor variables as well as with diabetes occurrence.\nNext, lets model. Click here to go to the Modeling section!"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "",
    "text": "In this project, we will be exploring the Diabetes Health Indicators Dataset (available on Kaggle).\nDiabetes is a serious chronic disease with multiple complications associated. There are many genetic, environmental, and lifestyle factors that can be associated with risk of diabetes. The risk of type II diabetes, the most common form, can differ by race, education, age, income, and many additional factors.\nThe data analyzed here was collected via a health-related telephone survey conducted by the CDC’s Behavioral Risk Factor Surveillance System (BRFSS). These features are either questions directly answered by participants or calculated variables based on participants’ responses. We are interested in finding the best model to predict the occurance of prediabetes or diabetes (a binary variable, Diabetes_binary). That is, we are interested in what risk factors are the most predictive of diabetes, and can we use this subset of variables to classify disease occurrence?\nThere are already well-known medical factors (mainly related to diet) that can increase risk of diabetes, so for this project I am going to focus on some factors that may have less-known associations; mental health, sex, age, education, income, and smoker status.\nEspecially because we did not personally create this data, the purpose of this exploratory analysis is to get to know the data and how it is stored. This includes overall distributions of variables, how variables are stored (numerically, as factor variables, etc- do the column types make sense for the variable) and any pre-existing relationships between variables that we should be aware of before attempting to fit the data to a predictive model.\nWith a well-fit model, we could potentially create a risk stratification panel for individuals that have not been diagnosed with prediabetes/diabetes.\nIn this file, we will perform exploratory data analysis before moving on to modeling with the data.\n\n\n\nClick here for the codebook!\n\n\nVariable\nData Type\nInformation\n\n\n\n\nDiabetes_binary\nbinary\n0 = no diabetes 1 = prediabetes or diabetes\n\n\nSmoker\nbinary\nHave you smoked at least 100 cigarettes in your entire life? 0 = no 1 = yes\n\n\nMentHlth\ninteger/numeric\nFor how many days during the past 30 days was your mental health not good? scale 1-30 days\n\n\nSex\nbinary\n0 = female 1 = male\n\n\nAge\ninteger\n13-level age category (_AGEG5YR see codebook) 1 = 18-24 9 = 60-64 13 = 80 or older\n\n\nEducation\ninteger/factor\nEducation level (EDUCA see codebook) scale 1-6\n\n\nIncome\ninteger/factor\nIncome scale (INCOME2 see codebook)"
  },
  {
    "objectID": "EDA.html#data-import",
    "href": "EDA.html#data-import",
    "title": "EDA",
    "section": "",
    "text": "diabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "EDA.html#exploratory-analysis-clean-up",
    "href": "EDA.html#exploratory-analysis-clean-up",
    "title": "EDA",
    "section": "",
    "text": "str(diabetes)\n\nspc_tbl_ [253,680 × 22] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Diabetes_binary     : num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num [1:253680] 1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num [1:253680] 1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num [1:253680] 1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num [1:253680] 0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num [1:253680] 1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num [1:253680] 0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num [1:253680] 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num [1:253680] 1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num [1:253680] 0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num [1:253680] 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Diabetes_binary = col_double(),\n  ..   HighBP = col_double(),\n  ..   HighChol = col_double(),\n  ..   CholCheck = col_double(),\n  ..   BMI = col_double(),\n  ..   Smoker = col_double(),\n  ..   Stroke = col_double(),\n  ..   HeartDiseaseorAttack = col_double(),\n  ..   PhysActivity = col_double(),\n  ..   Fruits = col_double(),\n  ..   Veggies = col_double(),\n  ..   HvyAlcoholConsump = col_double(),\n  ..   AnyHealthcare = col_double(),\n  ..   NoDocbcCost = col_double(),\n  ..   GenHlth = col_double(),\n  ..   MentHlth = col_double(),\n  ..   PhysHlth = col_double(),\n  ..   DiffWalk = col_double(),\n  ..   Sex = col_double(),\n  ..   Age = col_double(),\n  ..   Education = col_double(),\n  ..   Income = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# all variables have been imported as numeric, going to adjust appropriately- \n\ndiabetes &lt;- diabetes %&gt;% \n  mutate(Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n        Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n        Age = factor(Age, levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13), labels = c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80 or older\")),\n        Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"never attended school or only kindergarten\",\"elementary\",\"some high school\",\"high school graduate\",\"some college or technical school\",\"college graduate\")),\n        Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"less than 10k\",\"less than 15k\", \"less than 20k\", \"less than 25k\",\"less than 35k\",\"less than 50k\",\"less than 75k\",\"75 or more\"))\n         ) %&gt;% \n  select(c(Diabetes_binary, Smoker, Sex, Age, Education, Income, MentHlth))\n\n#Checking for missingness \nsum(is.na(diabetes))\n\n[1] 0\n\n  #no NAs \n\ndescribe(diabetes)\n\n                 vars      n mean   sd median trimmed  mad min max range  skew\nDiabetes_binary*    1 253680 1.14 0.35      1    1.05 0.00   1   2     1  2.08\nSmoker*             2 253680 1.44 0.50      1    1.43 0.00   1   2     1  0.23\nSex*                3 253680 1.44 0.50      1    1.43 0.00   1   2     1  0.24\nAge*                4 253680 8.03 3.05      8    8.17 2.97   1  13    12 -0.36\nEducation*          5 253680 5.05 0.99      5    5.15 1.48   1   6     5 -0.78\nIncome*             6 253680 6.05 2.07      7    6.35 1.48   1   8     7 -0.89\nMentHlth            7 253680 3.18 7.41      0    1.04 0.00   0  30    30  2.72\n                 kurtosis   se\nDiabetes_binary*     2.34 0.00\nSmoker*             -1.95 0.00\nSex*                -1.94 0.00\nAge*                -0.58 0.01\nEducation*           0.04 0.00\nIncome*             -0.28 0.00\nMentHlth             6.44 0.01\n\n #note, all the categorical variables have * next to the variable name."
  },
  {
    "objectID": "EDA.html#data-summarizations",
    "href": "EDA.html#data-summarizations",
    "title": "EDA",
    "section": "",
    "text": "diabetes %&gt;% group_by(Diabetes_binary) %&gt;% summarize(count=n())\n\n# A tibble: 2 × 2\n  Diabetes_binary  count\n  &lt;fct&gt;            &lt;int&gt;\n1 No              218334\n2 Yes              35346\n\n\n\ndiabetes %&gt;% group_by(Diabetes_binary,Smoker) %&gt;% summarize(count=n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary Smoker  count\n  &lt;fct&gt;           &lt;fct&gt;   &lt;int&gt;\n1 No              No     124228\n2 No              Yes     94106\n3 Yes             No      17029\n4 Yes             Yes     18317\n\nggplot(data = diabetes, aes(x=Smoker, fill=Diabetes_binary)) +\n  geom_bar() +\n  labs(x=\"Smoker Status\") +\n  scale_fill_discrete(\"Diabetes\")\n\n\n\n\nThere doesn’t appear to be any notable association here.\n\ndiabetes %&gt;% group_by(Diabetes_binary,Age) %&gt;% summarize(count=n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 26 × 3\n# Groups:   Diabetes_binary [2]\n   Diabetes_binary Age   count\n   &lt;fct&gt;           &lt;fct&gt; &lt;int&gt;\n 1 No              18-24  5622\n 2 No              25-29  7458\n 3 No              30-34 10809\n 4 No              35-39 13197\n 5 No              40-44 15106\n 6 No              45-49 18077\n 7 No              50-54 23226\n 8 No              55-59 26569\n 9 No              60-64 27511\n10 No              65-69 25636\n# ℹ 16 more rows\n\nggplot(data = diabetes, aes(x=Age, fill=Diabetes_binary)) +\n  geom_bar() +\n  labs(x=\"Age, 5 year bins\") +\n  scale_fill_discrete(\"Diabetes\")\n\n\n\n\nI expected diabetes occurance to have the positive trend with age. It visually looks like it goes down at 70+ years old, but I believe this is just due to the total N of these age groups being lower.\n\ndiabetes %&gt;% group_by(Diabetes_binary,Sex) %&gt;% summarize(count=n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary Sex     count\n  &lt;fct&gt;           &lt;fct&gt;   &lt;int&gt;\n1 No              Female 123563\n2 No              Male    94771\n3 Yes             Female  18411\n4 Yes             Male    16935\n\nggplot(data = diabetes, aes(x=Sex, fill=Diabetes_binary)) +\n  geom_bar() +\n  labs(x=\"Sex\") +\n  scale_fill_discrete(\"Diabetes\")\n\n\n\n\nNo big difference here.\n\ndiabetes %&gt;% group_by(Diabetes_binary,Income) %&gt;% summarize(count=n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 16 × 3\n# Groups:   Diabetes_binary [2]\n   Diabetes_binary Income        count\n   &lt;fct&gt;           &lt;fct&gt;         &lt;int&gt;\n 1 No              less than 10k  7428\n 2 No              less than 15k  8697\n 3 No              less than 20k 12426\n 4 No              less than 25k 16081\n 5 No              less than 35k 21379\n 6 No              less than 50k 31179\n 7 No              less than 75k 37954\n 8 No              75 or more    83190\n 9 Yes             less than 10k  2383\n10 Yes             less than 15k  3086\n11 Yes             less than 20k  3568\n12 Yes             less than 25k  4054\n13 Yes             less than 35k  4504\n14 Yes             less than 50k  5291\n15 Yes             less than 75k  5265\n16 Yes             75 or more     7195\n\nggplot(data = diabetes, aes(x=Income, fill=Diabetes_binary)) +\n  geom_bar() +\n  labs(x=\"Income\") +\n  scale_fill_discrete(\"Diabetes\")\n\n\n\n\nOne thing you could glean here is that there are over double the amount of people that make more than 75k versus less than 75k (and every other wealth status for that matter) but it doesn’t appear that diabetes occurance doubles- looks like income could potentially be a protective factor to a certain extent. This would make sense; better access to healthier foods, health care, etc.\n\ndiabetes %&gt;% group_by(Diabetes_binary,Education) %&gt;% summarize(count=n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 12 × 3\n# Groups:   Diabetes_binary [2]\n   Diabetes_binary Education                                  count\n   &lt;fct&gt;           &lt;fct&gt;                                      &lt;int&gt;\n 1 No              never attended school or only kindergarten   127\n 2 No              elementary                                  2860\n 3 No              some high school                            7182\n 4 No              high school graduate                       51684\n 5 No              some college or technical school           59556\n 6 No              college graduate                           96925\n 7 Yes             never attended school or only kindergarten    47\n 8 Yes             elementary                                  1183\n 9 Yes             some high school                            2296\n10 Yes             high school graduate                       11066\n11 Yes             some college or technical school           10354\n12 Yes             college graduate                           10400\n\nggplot(data = diabetes, aes(x=Education, fill=Diabetes_binary)) +\n  geom_bar() +\n  labs(x=\"Education\") +\n  scale_fill_discrete(\"Diabetes\")\n\n\n\n\nNo notable associatons here. I wish they were able to collect a more equal amount of people across all education statuses.\n\ndiabetes %&gt;% group_by(Diabetes_binary) %&gt;% summarize(mean_MentHlth=mean(MentHlth),median_MentHlth=median(MentHlth))\n\n# A tibble: 2 × 3\n  Diabetes_binary mean_MentHlth median_MentHlth\n  &lt;fct&gt;                   &lt;dbl&gt;           &lt;dbl&gt;\n1 No                       2.98               0\n2 Yes                      4.46               0\n\nggplot(data = diabetes) +\n  geom_boxplot(aes(x = Diabetes_binary, y=MentHlth, fill=Diabetes_binary)) +\n  labs(y=\"Mental Health, Rated 1-30 Days Feeling Poorly\")\n\n\n\n\nIt looks like those with diabetes have experienced more poor mental health days than those without, which makes sense. It would be interesting to see that variable further split up- what caused that poor mental health (job? physical health? relationship stress?)\n\n\n\n\nggplot(data=diabetes) +\n  geom_boxplot(aes(x=Income, y=MentHlth, fill = Income))\n\n\n\n\nIt looks like the median of poor mental health days decreases as income levels increase. Lower-income groups have a higher spread/variability than higher-income groups. In general, higher income appears to correlate with fewer days of poor mental health on average.\n\nggplot(data=diabetes) +\n  geom_boxplot(aes(x=Education, y=MentHlth, fill = Education))\n\n\n\n\nWe see similar trends here; it appears that the median amount of poor days decreases as education levels increase.\n\nmosaicplot(table(diabetes$Income, diabetes$Education), \n           color = TRUE, \n           main = \"Mosaic Plot of Income and Education\",\n           xlab = \"Income\", \n           ylab = \"Education\")\n\n\n\n\nEducation and income appear to have a positive correlation.\n\nmosaicplot(table(diabetes$Smoker, diabetes$Sex), \n           color = TRUE, \n           main = \"Mosaic Plot of Smoker status and Sex\",\n           xlab = \"Smoker\", \n           ylab = \"Sex\")\n\n\n\n\nIn this population if you are a male you are more likely to be a smoker than if you are a female.\n\nggplot(data = diabetes, aes(x=Sex, fill =Smoker)) +\n  geom_bar(position = \"dodge\") +\n  labs(x=\"Sex\") +\n  scale_fill_discrete(\"Smoker\") +\n  facet_wrap(~ Education)\n\n\n\n\nHere we see that for lower education levels (elementary, some highschool, highschool graduate, some college) there are more male smokers than female (relative to the total count in each group- there is a higher percentage of men that smoke that dont) but in the highest education level, for both sexes it is less common to be a smoker.\n\nggplot(data = diabetes, aes(x=Sex, fill =Smoker)) +\n  geom_bar(position = \"dodge\") +\n  labs(x=\"Sex\") +\n  scale_fill_discrete(\"Smoker\") +\n  facet_wrap(~ Income)\n\n\n\n\nWe see the same trends here- again highlighting the association between income and education it appears."
  },
  {
    "objectID": "EDA.html#conclusions",
    "href": "EDA.html#conclusions",
    "title": "EDA",
    "section": "",
    "text": "The biggest thing we have gathered from this analysis is that there is an association between two predictor variables, education and income. There are likely other positive associations that just make logical sense (like age and smoking, or age and education or income level). We also saw some potential associations between mental health and other predictor variables as well as with diabetes occurrence.\nNext, lets model. Click here to go to the Modeling section!"
  }
]