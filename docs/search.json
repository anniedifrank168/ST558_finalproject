[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "In this project, we will be exploring the Diabetes Health Indicators Dataset (available on Kaggle).\nThe data analyzed here was collected via a health-related telephone survey conducted by the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) in 2015. We are interested in finding the best model to predict the occurance of prediabetes or diabetes (a binary variable, Diabetes_binary). That is, we are interested in what risk factors are the most predictive of diabetes, and can we use this subset of variables to classify disease occurrence?\nThere are already well-known medical factors (mainly related to diet) that can increase risk of diabetes, so for this project I am going to focus on some factors that may have less-known associations; mental health, sex, age, education, income, and smoker status.\nThere are many ways to fit a model to a dataset. Here, we split our data into training and test sets and use cross validation to improve the variability of the model performance from one data set to the next. We will look at classification tree modeling and random forest modeling and pick the best model from our options.\nWith a well-fit model, we could potentially create a risk stratification panel for individuals that have not been diagnosed with prediabetes/diabetes.\nIn this file, we will perform perform our model split and fit the data to multiple models with cross validation.\n\n\n\n\n diabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n diabetes &lt;- diabetes %&gt;% \n  mutate(Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n        Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n        Age = factor(Age, levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13), labels = c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80 or older\")),\n        Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"never attended school or only kindergarten\",\"elementary\",\"some high school\",\"high school graduate\",\"some college or technical school\",\"college graduate\")),\n        Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"less than 10k\",\"less than 15k\", \"less than 20k\", \"less than 25k\",\"less than 35k\",\"less than 50k\",\"less than 75k\",\"75 or more\"))\n         ) %&gt;% \n  select(c(Diabetes_binary, Smoker, Sex, Age, Education, Income, MentHlth))\n\n\n\n\nHere, we will use the tidymodels frame work to split our data into training and test sets, with a 0.7/0.3 proportion. Luckily we have many many observations, so we shouldn’t have to worry much about too little data being in the training or test split.\n\nset.seed(10)\ndiab_split&lt;- initial_split(diabetes,prop=0.7)\ndiab_train&lt;- training(diab_split)\ndiab_test&lt;- testing(diab_split)\ndiab_train\n\n# A tibble: 177,576 × 7\n   Diabetes_binary Smoker Sex    Age   Education                 Income MentHlth\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;                     &lt;fct&gt;     &lt;dbl&gt;\n 1 No              No     Male   40-44 college graduate          75 or…        0\n 2 No              No     Female 35-39 college graduate          75 or…        0\n 3 No              Yes    Female 50-54 some college or technica… 75 or…        0\n 4 No              Yes    Male   70-74 high school graduate      less …        0\n 5 No              No     Female 65-69 college graduate          less …        0\n 6 No              No     Male   50-54 some college or technica… 75 or…        3\n 7 No              No     Female 25-29 college graduate          75 or…        2\n 8 No              Yes    Male   50-54 college graduate          75 or…        2\n 9 No              Yes    Female 45-49 some college or technica… less …        0\n10 Yes             No     Female 65-69 college graduate          75 or…        0\n# ℹ 177,566 more rows\n\n\n\n\n\nWhen we split our data, it is randomly split intro training/test. There are chances we may get a weird split by chance, which can make metric evaluation a somewhat variable measurement. Using cross validation can create a less variable measurement of our metric that uses all of our data. With cross validation, no predictions used in the value of the metric were found on data that were used to train that model! Here, we will perform a 5-fold cross validation.\n\ndiab_cv_folds &lt;- vfold_cv(diab_train, 5)\n\n\n\n\nTree models are nonlinear supervised learning models and can be more flexible than linear models. Classification trees, as opposed to regression trees, are used when the response variable is categorical (or binary). With a tree model, we split the predictor space into regions. We than make our prediction based on which ‘bin’ an observation ends up in. The most prevalent class in a bin/region is used as the prediction at that split. These models are easy to interpret, and there are no statistical assumptions necessary to get the fit. Additionally, it has built in variable selection. The con of trees is that small changes in the data can vastly change the tree due to the lack of ‘sharing’ information with nearby data points due to splitting at bins, and there is no optimal algorithm for choosing splits. We also need to use CV to prune the tree/ figure out the optimal size of the tree (a bigger tree may be better, but the computational cost is large).\n\n#Creating our recipe \ntree_rec &lt;- recipe(Diabetes_binary ~., data = diab_train) %&gt;% \n  step_dummy(Age,Income,Sex,Education,Smoker) %&gt;% #create dummy variables for our categorical variables \n  step_normalize(MentHlth) #normalize our numeric variable \n\n#Defining our model and engine\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n=20,\n                          cost_complexity = tune()) %&gt;% \n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n#Creating our workflow \ntree_wkf &lt;- workflow() %&gt;% \n  add_recipe(tree_rec) %&gt;% \n  add_model(tree_mod)\n\n#Using CV to select tuning parameter \ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(2,2))\n\ntree_metrics &lt;- metric_set(mn_log_loss,accuracy) \n\ntree_fits &lt;- tree_wkf %&gt;% \n  tune_grid(resamples = diab_cv_folds,\n            grid = tree_grid,\n            metrics = tree_metrics)\n\ntree_fits %&gt;% collect_metrics() %&gt;% filter(.metric == \"mn_log_loss\")\n\n# A tibble: 4 × 8\n  cost_complexity tree_depth .metric     .estimator  mean     n  std_err .config\n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n1    0.0000000001          1 mn_log_loss binary     0.404     5 0.000720 Prepro…\n2    0.1                   1 mn_log_loss binary     0.404     5 0.000720 Prepro…\n3    0.0000000001         15 mn_log_loss binary     0.379     5 0.000629 Prepro…\n4    0.1                  15 mn_log_loss binary     0.404     5 0.000720 Prepro…\n\n#Selecting the best model's tuning parameters \ntree_best_param&lt;- select_best(tree_fits)\n\nWarning in select_best(tree_fits): No value of `metric` was given;\n\"mn_log_loss\" will be used.\n\n#Finalize the workflow with the best parameters \ntree_final_wkf &lt;- tree_wkf %&gt;% \n  finalize_workflow(tree_best_param)\n\n#Now that we've set up how to fit the final model, will do it on the split object \ntree_final_fit &lt;- tree_final_wkf %&gt;% \n  last_fit(diab_split, metrics = metric_set(accuracy, mn_log_loss))\n\ntree_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.859 Preprocessor1_Model1\n2 mn_log_loss binary         0.380 Preprocessor1_Model1\n\n\nExtracting and our final tree model and examining the variable importance plot:\n\ntree_final_model &lt;- extract_workflow(tree_final_fit) %&gt;% extract_fit_parsnip()\nvip(tree_final_model, geom = \"col\")\n\n\n\n\nIt looks like the biggest variables effecting our model are related to income, mental health, and age. I suspect that income has such a big play as greater income means greater access to healthy foods, health care, and maybe could correlate to be a better work-life balance (access to adequate sleep, exercise, etc.)\n\n\n\nRandom forest models are an ensemble method, meaning they combine many models together and fit multiple different tree models to pseudo replicates of the data set and then combine the outcomes. This method often uses bootstrapping to get multiple models to fit on. This can decrease the variance over an individual tree fit. As mentioned prior, with a basic classification or regression tree model, a branch/split we get can look vastly different from one tree to the next (from one dataset to the next, splits may look very different). Averaging across multiple trees, getting the variance of an average y, improves consistency.\nRandom forest models use bagging (bootstrap aggregation) to create many bootstrap samples, average the bootstrap statistics, and create a bootstrap distribution that mimics the sampling distribution. We randomly select the predictors to use at each split, which decreases correlation between trees. We fit trees to each resample and find predicted y for each, and for classification trees, use the majority vote of the predictions (most common prediction made by all bootstrap trees).\nAveraging many trees can greatly improve prediction, but comes at a loss of interpretability as opposed to normal classification trees.\nIn our model, we will tune on the number of predictors that each split should use. Using fewer predictors at each split will reduce the variance in each tree, but may increase the bias. Using more will reduce bias, but may increase variance and potentially lead to overfitting.\n\n#Using the same recipe as our previous model \nrf_spec &lt;- rand_forest(mtry= tune()) %&gt;% \n  set_engine(\"ranger\", importance = \"permutation\") %&gt;% \n  set_mode(\"classification\")\n\n#Creating workflow \nrf_wkf &lt;- workflow() %&gt;% \n  add_recipe(tree_rec) %&gt;% \n  add_model(rf_spec)\n  \n#Tuning mtry across 5 levels \nrf_grid &lt;- grid_regular(mtry(range = c(3,6)), levels = 2)\n#Fitting to our CV folds \nrf_ft &lt;- rf_wkf %&gt;% \n  tune_grid(resamples = diab_cv_folds,\n            grid = rf_grid,\n            metrics = metric_set(accuracy, mn_log_loss)) #this took about 20 min to run\n\nrf_ft %&gt;% collect_metrics() %&gt;% filter(.metric == \"mn_log_loss\")\n\n# A tibble: 2 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     3 mn_log_loss binary     0.373     5 0.000527 Preprocessor1_Model1\n2     6 mn_log_loss binary     0.370     5 0.000546 Preprocessor1_Model2\n\n#Extract best tuning parameter \nrf_best_param &lt;- select_best(rf_ft, metric = \"mn_log_loss\")\nrf_best_param #The best mtry is 6 predictors \n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     6 Preprocessor1_Model2\n\nrf_final_wkf &lt;- rf_wkf %&gt;% \n  finalize_workflow(rf_best_param)\n\n#refit on the entire training set using this parameter \nrf_final_fit &lt;- rf_final_wkf %&gt;% \n  last_fit(diab_split, metrics = metric_set(accuracy, mn_log_loss))\nrf_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.371 Preprocessor1_Model1\n\n\nExtracting and our final random forest tree model and examining the variable importance plot:\n\n#extract the final model and plot \nrf_final_model &lt;- extract_fit_engine(rf_final_fit)\n\n#VIP\nimpor_data&lt;- as.data.frame(rf_final_model$variable.importance) %&gt;% rownames_to_column(\"Variable\") %&gt;%\n  rename(value = \"rf_final_model$variable.importance\") %&gt;% \n  arrange(desc(value)) \n\nggplot(impor_data, aes(x = reorder(Variable, value), y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() \n\n\n\n\n\n\n\nHere we will compare our best model from the random forest method and the classification tree method on the test set to declare an overall best model.\n\ntree_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.859 Preprocessor1_Model1\n2 mn_log_loss binary         0.380 Preprocessor1_Model1\n\nrf_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.371 Preprocessor1_Model1\n\n\nOur best model using a classification tree had a mean log loss of 0.380 with a tree depth of 15 and cost complexity 1e-10, and declared incoming being greater than $75,000 annually has the most important predictor in the model.\nOur best model using a random forest tree had a mean log loss of 0.371 with mtry = 6 and declared being a college graduate (category of the Education variable) having the highest importance as a predictor, with income greater than $75,000 the second most important.\nOur overall best model, determined via the mean log loss metric, is the random forest tree model."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "",
    "text": "In this project, we will be exploring the Diabetes Health Indicators Dataset (available on Kaggle).\nThe data analyzed here was collected via a health-related telephone survey conducted by the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) in 2015. We are interested in finding the best model to predict the occurance of prediabetes or diabetes (a binary variable, Diabetes_binary). That is, we are interested in what risk factors are the most predictive of diabetes, and can we use this subset of variables to classify disease occurrence?\nThere are already well-known medical factors (mainly related to diet) that can increase risk of diabetes, so for this project I am going to focus on some factors that may have less-known associations; mental health, sex, age, education, income, and smoker status.\nThere are many ways to fit a model to a dataset. Here, we split our data into training and test sets and use cross validation to improve the variability of the model performance from one data set to the next. We will look at classification tree modeling and random forest modeling and pick the best model from our options.\nWith a well-fit model, we could potentially create a risk stratification panel for individuals that have not been diagnosed with prediabetes/diabetes.\nIn this file, we will perform perform our model split and fit the data to multiple models with cross validation."
  },
  {
    "objectID": "Modeling.html#data-import-clean-up",
    "href": "Modeling.html#data-import-clean-up",
    "title": "Modeling",
    "section": "",
    "text": "diabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n diabetes &lt;- diabetes %&gt;% \n  mutate(Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n        Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n        Age = factor(Age, levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13), labels = c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80 or older\")),\n        Education = factor(Education, levels = c(1,2,3,4,5,6), labels = c(\"never attended school or only kindergarten\",\"elementary\",\"some high school\",\"high school graduate\",\"some college or technical school\",\"college graduate\")),\n        Income = factor(Income, levels = c(1,2,3,4,5,6,7,8), labels = c(\"less than 10k\",\"less than 15k\", \"less than 20k\", \"less than 25k\",\"less than 35k\",\"less than 50k\",\"less than 75k\",\"75 or more\"))\n         ) %&gt;% \n  select(c(Diabetes_binary, Smoker, Sex, Age, Education, Income, MentHlth))"
  },
  {
    "objectID": "Modeling.html#splitting-the-data",
    "href": "Modeling.html#splitting-the-data",
    "title": "Modeling",
    "section": "",
    "text": "Here, we will use the tidymodels frame work to split our data into training and test sets, with a 0.7/0.3 proportion. Luckily we have many many observations, so we shouldn’t have to worry much about too little data being in the training or test split.\n\nset.seed(10)\ndiab_split&lt;- initial_split(diabetes,prop=0.7)\ndiab_train&lt;- training(diab_split)\ndiab_test&lt;- testing(diab_split)\ndiab_train\n\n# A tibble: 177,576 × 7\n   Diabetes_binary Smoker Sex    Age   Education                 Income MentHlth\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;                     &lt;fct&gt;     &lt;dbl&gt;\n 1 No              No     Male   40-44 college graduate          75 or…        0\n 2 No              No     Female 35-39 college graduate          75 or…        0\n 3 No              Yes    Female 50-54 some college or technica… 75 or…        0\n 4 No              Yes    Male   70-74 high school graduate      less …        0\n 5 No              No     Female 65-69 college graduate          less …        0\n 6 No              No     Male   50-54 some college or technica… 75 or…        3\n 7 No              No     Female 25-29 college graduate          75 or…        2\n 8 No              Yes    Male   50-54 college graduate          75 or…        2\n 9 No              Yes    Female 45-49 some college or technica… less …        0\n10 Yes             No     Female 65-69 college graduate          75 or…        0\n# ℹ 177,566 more rows"
  },
  {
    "objectID": "Modeling.html#cross-validation-folding",
    "href": "Modeling.html#cross-validation-folding",
    "title": "Modeling",
    "section": "",
    "text": "When we split our data, it is randomly split intro training/test. There are chances we may get a weird split by chance, which can make metric evaluation a somewhat variable measurement. Using cross validation can create a less variable measurement of our metric that uses all of our data. With cross validation, no predictions used in the value of the metric were found on data that were used to train that model! Here, we will perform a 5-fold cross validation.\n\ndiab_cv_folds &lt;- vfold_cv(diab_train, 5)"
  },
  {
    "objectID": "Modeling.html#classification-tree-modeling",
    "href": "Modeling.html#classification-tree-modeling",
    "title": "Modeling",
    "section": "",
    "text": "Tree models are nonlinear supervised learning models and can be more flexible than linear models. Classification trees, as opposed to regression trees, are used when the response variable is categorical (or binary). With a tree model, we split the predictor space into regions. We than make our prediction based on which ‘bin’ an observation ends up in. The most prevalent class in a bin/region is used as the prediction at that split. These models are easy to interpret, and there are no statistical assumptions necessary to get the fit. Additionally, it has built in variable selection. The con of trees is that small changes in the data can vastly change the tree due to the lack of ‘sharing’ information with nearby data points due to splitting at bins, and there is no optimal algorithm for choosing splits. We also need to use CV to prune the tree/ figure out the optimal size of the tree (a bigger tree may be better, but the computational cost is large).\n\n#Creating our recipe \ntree_rec &lt;- recipe(Diabetes_binary ~., data = diab_train) %&gt;% \n  step_dummy(Age,Income,Sex,Education,Smoker) %&gt;% #create dummy variables for our categorical variables \n  step_normalize(MentHlth) #normalize our numeric variable \n\n#Defining our model and engine\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n=20,\n                          cost_complexity = tune()) %&gt;% \n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n#Creating our workflow \ntree_wkf &lt;- workflow() %&gt;% \n  add_recipe(tree_rec) %&gt;% \n  add_model(tree_mod)\n\n#Using CV to select tuning parameter \ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(2,2))\n\ntree_metrics &lt;- metric_set(mn_log_loss,accuracy) \n\ntree_fits &lt;- tree_wkf %&gt;% \n  tune_grid(resamples = diab_cv_folds,\n            grid = tree_grid,\n            metrics = tree_metrics)\n\ntree_fits %&gt;% collect_metrics() %&gt;% filter(.metric == \"mn_log_loss\")\n\n# A tibble: 4 × 8\n  cost_complexity tree_depth .metric     .estimator  mean     n  std_err .config\n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n1    0.0000000001          1 mn_log_loss binary     0.404     5 0.000720 Prepro…\n2    0.1                   1 mn_log_loss binary     0.404     5 0.000720 Prepro…\n3    0.0000000001         15 mn_log_loss binary     0.379     5 0.000629 Prepro…\n4    0.1                  15 mn_log_loss binary     0.404     5 0.000720 Prepro…\n\n#Selecting the best model's tuning parameters \ntree_best_param&lt;- select_best(tree_fits)\n\nWarning in select_best(tree_fits): No value of `metric` was given;\n\"mn_log_loss\" will be used.\n\n#Finalize the workflow with the best parameters \ntree_final_wkf &lt;- tree_wkf %&gt;% \n  finalize_workflow(tree_best_param)\n\n#Now that we've set up how to fit the final model, will do it on the split object \ntree_final_fit &lt;- tree_final_wkf %&gt;% \n  last_fit(diab_split, metrics = metric_set(accuracy, mn_log_loss))\n\ntree_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.859 Preprocessor1_Model1\n2 mn_log_loss binary         0.380 Preprocessor1_Model1\n\n\nExtracting and our final tree model and examining the variable importance plot:\n\ntree_final_model &lt;- extract_workflow(tree_final_fit) %&gt;% extract_fit_parsnip()\nvip(tree_final_model, geom = \"col\")\n\n\n\n\nIt looks like the biggest variables effecting our model are related to income, mental health, and age. I suspect that income has such a big play as greater income means greater access to healthy foods, health care, and maybe could correlate to be a better work-life balance (access to adequate sleep, exercise, etc.)"
  },
  {
    "objectID": "Modeling.html#random-forest-modeling",
    "href": "Modeling.html#random-forest-modeling",
    "title": "Modeling",
    "section": "",
    "text": "Random forest models are an ensemble method, meaning they combine many models together and fit multiple different tree models to pseudo replicates of the data set and then combine the outcomes. This method often uses bootstrapping to get multiple models to fit on. This can decrease the variance over an individual tree fit. As mentioned prior, with a basic classification or regression tree model, a branch/split we get can look vastly different from one tree to the next (from one dataset to the next, splits may look very different). Averaging across multiple trees, getting the variance of an average y, improves consistency.\nRandom forest models use bagging (bootstrap aggregation) to create many bootstrap samples, average the bootstrap statistics, and create a bootstrap distribution that mimics the sampling distribution. We randomly select the predictors to use at each split, which decreases correlation between trees. We fit trees to each resample and find predicted y for each, and for classification trees, use the majority vote of the predictions (most common prediction made by all bootstrap trees).\nAveraging many trees can greatly improve prediction, but comes at a loss of interpretability as opposed to normal classification trees.\nIn our model, we will tune on the number of predictors that each split should use. Using fewer predictors at each split will reduce the variance in each tree, but may increase the bias. Using more will reduce bias, but may increase variance and potentially lead to overfitting.\n\n#Using the same recipe as our previous model \nrf_spec &lt;- rand_forest(mtry= tune()) %&gt;% \n  set_engine(\"ranger\", importance = \"permutation\") %&gt;% \n  set_mode(\"classification\")\n\n#Creating workflow \nrf_wkf &lt;- workflow() %&gt;% \n  add_recipe(tree_rec) %&gt;% \n  add_model(rf_spec)\n  \n#Tuning mtry across 5 levels \nrf_grid &lt;- grid_regular(mtry(range = c(3,6)), levels = 2)\n#Fitting to our CV folds \nrf_ft &lt;- rf_wkf %&gt;% \n  tune_grid(resamples = diab_cv_folds,\n            grid = rf_grid,\n            metrics = metric_set(accuracy, mn_log_loss)) #this took about 20 min to run\n\nrf_ft %&gt;% collect_metrics() %&gt;% filter(.metric == \"mn_log_loss\")\n\n# A tibble: 2 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     3 mn_log_loss binary     0.373     5 0.000527 Preprocessor1_Model1\n2     6 mn_log_loss binary     0.370     5 0.000546 Preprocessor1_Model2\n\n#Extract best tuning parameter \nrf_best_param &lt;- select_best(rf_ft, metric = \"mn_log_loss\")\nrf_best_param #The best mtry is 6 predictors \n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     6 Preprocessor1_Model2\n\nrf_final_wkf &lt;- rf_wkf %&gt;% \n  finalize_workflow(rf_best_param)\n\n#refit on the entire training set using this parameter \nrf_final_fit &lt;- rf_final_wkf %&gt;% \n  last_fit(diab_split, metrics = metric_set(accuracy, mn_log_loss))\nrf_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.371 Preprocessor1_Model1\n\n\nExtracting and our final random forest tree model and examining the variable importance plot:\n\n#extract the final model and plot \nrf_final_model &lt;- extract_fit_engine(rf_final_fit)\n\n#VIP\nimpor_data&lt;- as.data.frame(rf_final_model$variable.importance) %&gt;% rownames_to_column(\"Variable\") %&gt;%\n  rename(value = \"rf_final_model$variable.importance\") %&gt;% \n  arrange(desc(value)) \n\nggplot(impor_data, aes(x = reorder(Variable, value), y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip()"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling",
    "section": "",
    "text": "Here we will compare our best model from the random forest method and the classification tree method on the test set to declare an overall best model.\n\ntree_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.859 Preprocessor1_Model1\n2 mn_log_loss binary         0.380 Preprocessor1_Model1\n\nrf_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.371 Preprocessor1_Model1\n\n\nOur best model using a classification tree had a mean log loss of 0.380 with a tree depth of 15 and cost complexity 1e-10, and declared incoming being greater than $75,000 annually has the most important predictor in the model.\nOur best model using a random forest tree had a mean log loss of 0.371 with mtry = 6 and declared being a college graduate (category of the Education variable) having the highest importance as a predictor, with income greater than $75,000 the second most important.\nOur overall best model, determined via the mean log loss metric, is the random forest tree model."
  }
]